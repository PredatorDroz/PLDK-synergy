{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Image Classifier with Differential Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "MAX_GRAD_NORM = 1\n",
    "EPSILON = 190\n",
    "DELTA = 1e-5\n",
    "EPOCHS = 20\n",
    "\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "MAX_PHYSICAL_BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the CIFAR100 dataset. We don't use data augmentation here because, in our experiments, we found that data augmentation lowers utility when training with DP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "# These values, specific to the CIFAR10 dataset, are assumed to be known.\n",
    "# If necessary, they can be computed with modest privacy budgets.\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "DATA_ROOT = r\"F:\\Other\\data\"\n",
    "\n",
    "train_ds = datasets.CIFAR100(os.path.join(DATA_ROOT, 'cifar100'), train=True, download=True)\n",
    "test_ds = datasets.CIFAR100(os.path.join(DATA_ROOT, 'cifar100'), train=False, download=True)\n",
    "\n",
    "_data_train = np.concatenate([np.array(train_ds[i][0]) for i in range(len(train_ds))])\n",
    "_data_test = np.concatenate([np.array(test_ds[i][0]) for i in range(len(test_ds))])\n",
    "\n",
    "train_mean = _data_train.mean(axis=(0, 1))\n",
    "train_std = _data_train.std(axis=(0, 1))\n",
    "\n",
    "test_mean = _data_test.mean(axis=(0, 1))\n",
    "test_std = _data_test.std(axis=(0, 1))\n",
    "\n",
    "\n",
    "\n",
    "# Normalize mean std to 0..1 from 0..255\n",
    "train_mean /= 255\n",
    "train_std /= 255\n",
    "test_mean /= 255\n",
    "test_std /= 255\n",
    "\n",
    "print('train mean/std:', train_mean, train_std)\n",
    "print('test mean/std:', test_mean, test_std)\n",
    "\n",
    "print(f'Hard code CIFAR100 train/test mean/std for next time')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(test_mean, test_std),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using torchvision datasets, we can load CIFAR10 and transform the PILImage images to Tensors of normalized range [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10,CIFAR100\n",
    "\n",
    "# DATA_ROOT = 'D:/research_2022/'\n",
    "\n",
    "train_dataset = CIFAR100(\n",
    "    root=DATA_ROOT, train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "# indices = torch.arange(35000)\n",
    "# train_dataset = data_utils.Subset(train_dataset, indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "test_dataset = CIFAR100(\n",
    "    root=DATA_ROOT, train=False, download=True, transform=transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' ConvNet '''\n",
    "import torch.nn as nn\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, channel, num_classes, net_width, net_depth, net_act, net_norm, net_pooling, im_size = (32,32)):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.features, shape_feat = self._make_layers(channel, net_width, net_depth, net_norm, net_act, net_pooling, im_size)\n",
    "        num_feat = shape_feat[0]*shape_feat[1]*shape_feat[2]\n",
    "        self.classifier = nn.Linear(num_feat, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"MODEL DATA ON: \", x.get_device(), \"MODEL PARAMS ON: \", self.classifier.weight.data.get_device())\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _get_activation(self, net_act):\n",
    "        if net_act == 'sigmoid':\n",
    "            return nn.Sigmoid()\n",
    "        elif net_act == 'relu':\n",
    "            return nn.ReLU(inplace=True)\n",
    "        elif net_act == 'leakyrelu':\n",
    "            return nn.LeakyReLU(negative_slope=0.01)\n",
    "        else:\n",
    "            exit('unknown activation function: %s'%net_act)\n",
    "\n",
    "    def _get_pooling(self, net_pooling):\n",
    "        if net_pooling == 'maxpooling':\n",
    "            return nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        elif net_pooling == 'avgpooling':\n",
    "            return nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        elif net_pooling == 'none':\n",
    "            return None\n",
    "        else:\n",
    "            exit('unknown net_pooling: %s'%net_pooling)\n",
    "\n",
    "    def _get_normlayer(self, net_norm, shape_feat):\n",
    "        # shape_feat = (c*h*w)\n",
    "        if net_norm == 'batchnorm':\n",
    "            return nn.BatchNorm2d(shape_feat[0], affine=True)\n",
    "        elif net_norm == 'layernorm':\n",
    "            return nn.LayerNorm(shape_feat, elementwise_affine=True)\n",
    "        elif net_norm == 'instancenorm':\n",
    "            return nn.GroupNorm(shape_feat[0], shape_feat[0], affine=True)\n",
    "        elif net_norm == 'groupnorm':\n",
    "            return nn.GroupNorm(4, shape_feat[0], affine=True)\n",
    "        elif net_norm == 'none':\n",
    "            return None\n",
    "        else:\n",
    "            exit('unknown net_norm: %s'%net_norm)\n",
    "\n",
    "    def _make_layers(self, channel, net_width, net_depth, net_norm, net_act, net_pooling, im_size):\n",
    "        layers = []\n",
    "        in_channels = channel\n",
    "        if im_size[0] == 28:\n",
    "            im_size = (32, 32)\n",
    "        shape_feat = [in_channels, im_size[0], im_size[1]]\n",
    "        for d in range(net_depth):\n",
    "            layers += [nn.Conv2d(in_channels, net_width, kernel_size=3, padding=3 if channel == 1 and d == 0 else 1)]\n",
    "            shape_feat[0] = net_width\n",
    "            if net_norm != 'none':\n",
    "                layers += [self._get_normlayer(net_norm, shape_feat)]\n",
    "            layers += [self._get_activation(net_act)]\n",
    "            in_channels = net_width\n",
    "            if net_pooling != 'none':\n",
    "                layers += [self._get_pooling(net_pooling)]\n",
    "                shape_feat[1] //= 2\n",
    "                shape_feat[2] //= 2\n",
    "\n",
    "\n",
    "        return nn.Sequential(*layers), shape_feat \n",
    "    \n",
    "    \n",
    "\n",
    "def get_default_convnet_setting():\n",
    "    net_width, net_depth, net_act, net_norm, net_pooling = 128, 3, 'relu', 'instancenorm', 'avgpooling'\n",
    "    return net_width, net_depth, net_act, net_norm, net_pooling\n",
    "\n",
    "net_width, net_depth, net_act, net_norm, net_pooling = get_default_convnet_setting()\n",
    "nets= ConvNet(channel=3, num_classes= 10,net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling,im_size=(32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% AlexNet Module\n",
    "\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=(5, 5), padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def alexnet(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet(**kwargs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "model =AlexNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s check if the model is compatible with Opacus. Opacus does not support all types of Pytorch layers. To check if your model is compatible with the privacy engine, we have provided a util class to validate your model.\n",
    "\n",
    "When you run the code below, you're presented with a list of errors, indicating which modules are incompatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opacus.validators import ModuleValidator\n",
    "\n",
    "errors = ModuleValidator.validate(model, strict=False)\n",
    "errors[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us modify the model to work with Opacus. From the output above, you can see that the BatchNorm layers are not supported because they compute the mean and variance across the batch, creating a dependency between samples in a batch, a privacy violation.\n",
    "\n",
    "Recommended approach to deal with it is calling `ModuleValidator.fix(model)` - it tries to find the best replacement for incompatible modules. For example, for BatchNorm modules, it replaces them with GroupNorm.\n",
    "You can see, that after this, no exception is raised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModuleValidator.fix(model)\n",
    "ModuleValidator.validate(model, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For maximal speed, we can check if CUDA is available and supported by the PyTorch installation. If GPU is available, set the `device` variable to your CUDA-compatible device. We can then transfer the neural network onto that device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define our optimizer and loss function. Opacus’ privacy engine can attach to any (first-order) optimizer.  You can use your favorite&mdash;Adam, Adagrad, RMSprop&mdash;as long as it has an implementation derived from [torch.optim.Optimizer](https://pytorch.org/docs/stable/optim.html). In this tutorial, we're going to use [RMSprop](https://pytorch.org/docs/stable/optim.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a util function to calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, labels):\n",
    "    return (preds == labels).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now attach the privacy engine initialized with the privacy hyperparameters defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opacus import PrivacyEngine\n",
    "\n",
    "privacy_engine = PrivacyEngine()\n",
    "\n",
    "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_loader,\n",
    "    epochs=EPOCHS,\n",
    "    target_epsilon=EPSILON,\n",
    "    target_delta=DELTA,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    ")\n",
    "\n",
    "print(f\"Using sigma={optimizer.noise_multiplier} and C={MAX_GRAD_NORM}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then define our train function. This function will train the model for one epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, epoch, device):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    losses = []\n",
    "    top1_acc = []\n",
    "    \n",
    "    with BatchMemoryManager(\n",
    "        data_loader=train_loader, \n",
    "        max_physical_batch_size=MAX_PHYSICAL_BATCH_SIZE, \n",
    "        optimizer=optimizer\n",
    "    ) as memory_safe_data_loader:\n",
    "\n",
    "        for i, (images, target) in enumerate(memory_safe_data_loader):   \n",
    "            optimizer.zero_grad()\n",
    "            images = images.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
    "            labels = target.detach().cpu().numpy()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc = accuracy(preds, labels)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            top1_acc.append(acc)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 200 == 0:\n",
    "                epsilon = privacy_engine.get_epsilon(DELTA)\n",
    "                print(\n",
    "                    f\"\\tTrain Epoch: {epoch} \\t\"\n",
    "                    f\"Loss: {np.mean(losses):.6f} \"\n",
    "                    f\"Acc@1: {np.mean(top1_acc) * 100:.6f} \"\n",
    "                    f\"(ε = {epsilon:.2f}, δ = {DELTA})\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define our test function to validate our model on our test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    top1_acc = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, target in test_loader:\n",
    "            images = images.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "            preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
    "            labels = target.detach().cpu().numpy()\n",
    "            acc = accuracy(preds, labels)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            top1_acc.append(acc)\n",
    "\n",
    "    top1_avg = np.mean(top1_acc)\n",
    "\n",
    "    print(\n",
    "        f\"\\tTest set:\"\n",
    "        f\"Loss: {np.mean(losses):.6f} \"\n",
    "        f\"Acc: {top1_avg * 100:.6f} \"\n",
    "    )\n",
    "    return np.mean(top1_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc=\"Epoch\", unit=\"epoch\"):\n",
    "    train(model, train_loader, optimizer, epoch + 1, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CIFAR100(\n",
    "    root=DATA_ROOT, train=False, download=True, transform=transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the network on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1_acc = test(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "# %% privacy_train\n",
    "# torch.Size([128, 3, 32, 32]) torch.Size([128]) torch.Size([128, 3, 32, 32]) torch.Size([128])\n",
    "# PRED torch.Size([256, 10])\n",
    "# infer_in torch.Size([256])\n",
    "def privacy_train(trainloader, model, inference_model, criterion, optimizer, use_cuda, num_batchs):\n",
    "    num_classes=10\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    mtop1_a = AverageMeter()\n",
    "    mtop5_a = AverageMeter()\n",
    "\n",
    "    inference_model.train()\n",
    "    model.eval()\n",
    "    # switch to evaluate mode\n",
    "\n",
    "    end = time.time()\n",
    "    first_id = -1\n",
    "    for batch_idx, ((tr_input, tr_target), (te_input, te_target)) in trainloader:\n",
    "        # measure data loading time\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        #print(tr_input.shape, tr_target.shape,te_input.shape, te_target.shape)\n",
    "\n",
    "        if use_cuda:\n",
    "            tr_input = tr_input.cuda()\n",
    "            te_input = te_input.cuda()\n",
    "            tr_target = tr_target.cuda()\n",
    "            te_target = te_target.cuda()\n",
    "\n",
    "        v_tr_input = torch.autograd.Variable(tr_input)\n",
    "        v_te_input = torch.autograd.Variable(te_input)\n",
    "        v_tr_target = torch.autograd.Variable(tr_target)\n",
    "        v_te_target = torch.autograd.Variable(te_target)\n",
    "\n",
    "        # compute output\n",
    "        model_input = torch.cat((v_tr_input, v_te_input))\n",
    "\n",
    "        pred_outputs = model(model_input)\n",
    "        #print(\"PRED\",pred_outputs.shape)\n",
    "        #y_hat\n",
    "\n",
    "        infer_input = torch.cat((v_tr_target, v_te_target))\n",
    "        #print(\"infer_in\",infer_input.shape)\n",
    "        #(y_hat)\n",
    "\n",
    "        # TODO fix\n",
    "        # mtop1, mtop5 = accuracy(pred_outputs.data, infer_input.data, topk=(1, 5))\n",
    "        mtop1 = top_k_accuracy_score(y_true=infer_input.data.cpu(), y_score=pred_outputs.data.cpu(),\n",
    "                                     k=1, labels=range(num_classes))\n",
    "\n",
    "        mtop5 = top_k_accuracy_score(y_true=infer_input.data.cpu(), y_score=pred_outputs.data.cpu(),\n",
    "                                     k=5, labels=range(num_classes))\n",
    "\n",
    "        mtop1_a.update(mtop1, model_input.size(0))\n",
    "        mtop5_a.update(mtop5, model_input.size(0))\n",
    "\n",
    "        one_hot_tr = torch.from_numpy((np.zeros((infer_input.size(0), num_classes)) - 1)).cuda().type(torch.float)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, infer_input.type(torch.int64).view([-1, 1]).data, 1)\n",
    "\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        #ONE_hot y_hat\n",
    "\n",
    "        attack_model_input = pred_outputs  # torch.cat((pred_outputs,infer_input_one_hot),1)\n",
    "        member_output = inference_model(attack_model_input, infer_input_one_hot)\n",
    "        #inf_model(y,y_hat)\n",
    "        #member->?0/1\n",
    "\n",
    "        is_member_labels = torch.from_numpy(\n",
    "            np.reshape(\n",
    "                np.concatenate((np.zeros(v_tr_input.size(0)), np.ones(v_te_input.size(0)))),\n",
    "                [-1, 1]\n",
    "            )\n",
    "        ).cuda()\n",
    "\n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels).type(torch.float)\n",
    "        #true_labels\n",
    "\n",
    "        loss = criterion(member_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = np.mean((member_output.data.cpu().numpy() > 0.5) == v_is_member_labels.data.cpu().numpy())\n",
    "        losses.update(loss.data.item(), model_input.size(0))\n",
    "        top1.update(prec1, model_input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx - first_id > num_batchs:\n",
    "            break\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx % 50 == 0:\n",
    "            #print(\"STUCK\")\n",
    "            print( losses.avg, top1.avg)\n",
    "            #print(report_str(batch_idx, data_time.avg, batch_time.avg, losses.avg, top1.avg, None))\n",
    "\n",
    "    return losses.avg, top1.avg\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "740px",
    "left": "0px",
    "right": "1628px",
    "top": "161px",
    "width": "253px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
