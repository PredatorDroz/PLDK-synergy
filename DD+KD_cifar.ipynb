{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a237b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main entrance for train/eval with/without KD on CIFAR-10\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utilse\n",
    "import model.net as net\n",
    "import model.data_loader as data_loader\n",
    "import model.resnet as resnet\n",
    "import model.wrn as wrn\n",
    "import model.densenet as densenet\n",
    "#import model.resnext as resnext\n",
    "import model.preresnet as preresnet\n",
    "from evaluate import evaluate, evaluate_kd\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--data_dir', default='data/64x64_SIGNS', help=\"Directory for the dataset\")\n",
    "parser.add_argument('--model_dir', default='experiments/base_model',\n",
    "                    help=\"Directory containing params.json\")\n",
    "parser.add_argument('--restore_file', default=None,\n",
    "                    help=\"Optional, name of the file in --model_dir \\\n",
    "                    containing weights to reload before training\")  # 'best' or 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d633122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.argv=\"examples/run_expt.py --dataset iwildcam --algorithm DANN --batch_size 8 --root_dir E:\\Python_project\\AdaCowd+Meta\\wilds-main\\wilds-main\\data --n_groups_per_batch 1 --distinct_group True --frac 0.01 --train_loader group --uniform_over_groups True\".split()\n",
    "sys.argv=\"train.py --model_dir experiments/cnn_distill\".split()\n",
    "\n",
    "# Print number of arguments passed in\n",
    "print (f'Number of arguments: {len(sys.argv)}')\n",
    "\n",
    "# Loop through the arguments and print them\n",
    "for arg in range(len(sys.argv)):\n",
    "  print(f' Argument {arg} is: {sys.argv[arg]}')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f051156",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the parameters from json file\n",
    "args = parser.parse_args()\n",
    "json_path = os.path.join(args.model_dir, 'params.json')\n",
    "assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "params = utilse.Params(json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3249c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model.alexnet as alexnets\n",
    "\n",
    "def train(model, optimizer, loss_fn, dataloader, metrics, params):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: \n",
    "        dataloader: \n",
    "        metrics: (dict) \n",
    "        params: (Params) hyperparameters\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    summ = []\n",
    "    loss_avg = utilse.RunningAverage()\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    with tqdm(total=len(dataloader)) as t:\n",
    "        for i, (train_batch, labels_batch) in enumerate(dataloader):\n",
    "            # move to GPU if available\n",
    "            #print(train_batch.shape)\n",
    "            if params.cuda:\n",
    "                train_batch, labels_batch = train_batch.cuda(non_blocking=False), labels_batch.cuda(non_blocking=False)\n",
    "                \n",
    "            # convert to torch Variables\n",
    "            train_batch, labels_batch = Variable(train_batch), Variable(labels_batch)\n",
    "\n",
    "            # compute model output and loss\n",
    "            output_batch = model(train_batch)\n",
    "            loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "            # clear previous gradients, compute gradients of all variables wrt loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # performs updates using calculated gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Evaluate summaries only once in a while\n",
    "            if i % params.save_summary_steps == 0:\n",
    "                # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "                output_batch = output_batch.data.cpu().numpy()\n",
    "                labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "                # compute all metrics on this batch\n",
    "                summary_batch = {metric:metrics[metric](output_batch, labels_batch)\n",
    "                                 for metric in metrics}\n",
    "                summary_batch['loss'] = loss.data.cpu()\n",
    "                summ.append(summary_batch)\n",
    "\n",
    "            # update the average loss\n",
    "            loss_avg.update(loss.data.cpu())\n",
    "\n",
    "            t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "            t.update()\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]}\n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    logging.info(\"- Train metrics: \" + metrics_string)\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, train_dataloader, val_dataloader, optimizer,\n",
    "                       loss_fn, metrics, params, model_dir, restore_file=None):\n",
    "    \"\"\"Train the model and evaluate every epoch.\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        params: (Params) hyperparameters\n",
    "        model_dir: (string) directory containing config, weights and log\n",
    "        restore_file: (string) - name of file to restore from (without its extension .pth.tar)\n",
    "    \"\"\"\n",
    "    save_dir = 'D:/research_2022/PyTorch-GAN-master/PyTorch-GAN-master/implementations/cgan/data/model/'\n",
    "    # reload weights from restore_file if specified\n",
    "    if restore_file is not None:\n",
    "        restore_path = os.path.join(args.model_dir, args.restore_file + '.pth.tar')\n",
    "        logging.info(\"Restoring parameters from {}\".format(restore_path))\n",
    "        utilse.load_checkpoint(restore_path, model, optimizer)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    # learning rate schedulers for different models:\n",
    "    if params.model_version == \"resnet18\":\n",
    "        scheduler = StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "    # for cnn models, num_epoch is always < 100, so it's intentionally not using scheduler here\n",
    "    elif params.model_version == \"cnn\":\n",
    "        scheduler = StepLR(optimizer, step_size=100, gamma=0.2)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=100, gamma=0.2)\n",
    "    for epoch in range(params.num_epochs):\n",
    "     \n",
    "        scheduler.step()\n",
    "     \n",
    "        # Run one epoch\n",
    "        logging.info(\"Epoch {}/{}\".format(epoch + 1, params.num_epochs))\n",
    "\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        train(model, optimizer, loss_fn, train_dataloader, metrics, params)\n",
    "\n",
    "        # Evaluate for one epoch on validation set\n",
    "        val_metrics = evaluate(model, loss_fn, val_dataloader, metrics, params)        \n",
    "\n",
    "        val_acc = val_metrics['accuracy']\n",
    "        is_best = val_acc>=best_val_acc\n",
    "\n",
    "        # Save weights\n",
    "        utilse.save_checkpoint({'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optim_dict' : optimizer.state_dict()},\n",
    "                               is_best=is_best,\n",
    "                               checkpoint=model_dir)\n",
    "       \n",
    "\n",
    "        # If best_eval, best_save_path\n",
    "        if is_best:\n",
    "            logging.info(\"- Found new best accuracy\")\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "            # Save best val metrics in a json file in the model directory\n",
    "            best_json_path = os.path.join(model_dir, \"metrics_val_best_weights.json\")\n",
    "            utilse.save_dict_to_json(val_metrics, best_json_path)\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, 'TeacherR101.pth')) \n",
    "\n",
    "        # Save latest val metrics in a json file in the model directory\n",
    "        last_json_path = os.path.join(model_dir, \"metrics_val_last_weights.json\")\n",
    "        utilse.save_dict_to_json(val_metrics, last_json_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8477615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining train_kd & train_and_evaluate_kd functions\n",
    "def train_kd(model, teacher_model, optimizer, loss_fn_kd, dataloader, metrics, params):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn_kd: \n",
    "        dataloader: \n",
    "        metrics: (dict) \n",
    "        params: (Params) hyperparameters\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "    teacher_model.eval()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    summ = []\n",
    "    loss_avg = utilse.RunningAverage()\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    with tqdm(total=len(dataloader)) as t:\n",
    "        for i, (train_batch, labels_batch) in enumerate(dataloader):\n",
    "            # move to GPU if available\n",
    "            if params.cuda:\n",
    "                train_batch, labels_batch = train_batch.cuda(non_blocking=False), \\\n",
    "                                            labels_batch.cuda(non_blocking=False)\n",
    "            # convert to torch Variables\n",
    "            train_batch, labels_batch = Variable(train_batch), Variable(labels_batch)\n",
    "\n",
    "            # compute model output, fetch teacher output, and compute KD loss\n",
    "            output_batch = model(train_batch)\n",
    "\n",
    "            # get one batch output from teacher_outputs list\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output_teacher_batch = teacher_model(train_batch)\n",
    "            if params.cuda:\n",
    "                output_teacher_batch = output_teacher_batch.cuda(non_blocking=False)\n",
    "\n",
    "            loss = loss_fn_kd(output_batch, labels_batch, output_teacher_batch, params)\n",
    "\n",
    "            # clear previous gradients, compute gradients of all variables wrt loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # performs updates using calculated gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Evaluate summaries only once in a while\n",
    "            if i % params.save_summary_steps == 0:\n",
    "                # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "                output_batch = output_batch.data.cpu().numpy()\n",
    "                labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "                # compute all metrics on this batch\n",
    "                summary_batch = {metric:metrics[metric](output_batch, labels_batch)\n",
    "                                 for metric in metrics}\n",
    "                summary_batch['loss'] = loss.data.cpu()\n",
    "                summ.append(summary_batch)\n",
    "\n",
    "            # update the average loss\n",
    "            loss_avg.update(loss.data.cpu())\n",
    "\n",
    "            t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "            t.update()\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]}\n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    logging.info(\"- Train metrics: \" + metrics_string)\n",
    "\n",
    "\n",
    "def train_and_evaluate_kd(model, teacher_model, train_dataloader, val_dataloader, optimizer,\n",
    "                       loss_fn_kd, metrics, params, model_dir, restore_file=None):\n",
    "    \"\"\"Train the model and evaluate every epoch.\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        params: (Params) hyperparameters\n",
    "        model_dir: (string) directory containing config, weights and log\n",
    "        restore_file: (string) - file to restore (without its extension .pth.tar)\n",
    "    \"\"\"\n",
    "    # reload weights from restore_file if specified\n",
    "    if restore_file is not None:\n",
    "        restore_path = os.path.join(args.model_dir, args.restore_file + '.pth.tar')\n",
    "        logging.info(\"Restoring parameters from {}\".format(restore_path))\n",
    "        utilse.load_checkpoint(restore_path, model, optimizer)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    # Tensorboard logger setup\n",
    "    # board_logger = utils.Board_Logger(os.path.join(model_dir, 'board_logs'))\n",
    "\n",
    "    # learning rate schedulers for different models:\n",
    "    if params.model_version == \"resnet18_distill\":\n",
    "        scheduler = StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "    # for cnn models, num_epoch is always < 100, so it's intentionally not using scheduler here\n",
    "    elif params.model_version == \"cnn_distill\": \n",
    "        scheduler = StepLR(optimizer, step_size=100, gamma=0.2) \n",
    "\n",
    "    for epoch in range(80):\n",
    "        scheduler.step()\n",
    "\n",
    "        # Run one epoch\n",
    "        logging.info(\"Epoch {}/{}\".format(epoch + 1, params.num_epochs))\n",
    "\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        train_kd(model, teacher_model, optimizer, loss_fn_kd, train_dataloader,\n",
    "                 metrics, params)\n",
    "\n",
    "        # Evaluate for one epoch on validation set\n",
    "        val_metrics = evaluate_kd(model, val_dataloader, metrics, params)\n",
    "\n",
    "        val_acc = val_metrics['accuracy']\n",
    "        is_best = val_acc>=best_val_acc\n",
    "\n",
    "        # Save weights\n",
    "        print(\"saving weights\")\n",
    "        utilse.save_checkpoint({'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optim_dict' : optimizer.state_dict()},\n",
    "                               is_best=is_best,\n",
    "                               checkpoint=model_dir)\n",
    "        utilse.save_checkpoint({'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optim_dict' : optimizer.state_dict()},\n",
    "                               is_best=is_best,\n",
    "                               checkpoint='experiments/base_cnn')\n",
    "\n",
    "        # If best_eval, best_save_path\n",
    "        if is_best:\n",
    "            logging.info(\"- Found new best accuracy\")\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "            # Save best val metrics in a json file in the model directory\n",
    "            best_json_path = os.path.join(model_dir, \"metrics_val_best_weights.json\")\n",
    "            utilse.save_dict_to_json(val_metrics, best_json_path)\n",
    "\n",
    "        # Save latest val metrics in a json file in the model directory\n",
    "        last_json_path = os.path.join(model_dir, \"metrics_val_last_weights.json\")\n",
    "        utilse.save_dict_to_json(val_metrics, last_json_path)\n",
    "\n",
    "\n",
    "        # #============ TensorBoard logging: uncomment below to turn in on ============#\n",
    "        # # (1) Log the scalar values\n",
    "        # info = {\n",
    "        #     'val accuracy': val_acc\n",
    "        # }\n",
    "\n",
    "        # for tag, value in info.items():\n",
    "        #     board_logger.scalar_summary(tag, value, epoch+1)\n",
    "\n",
    "        # # (2) Log values and gradients of the parameters (histogram)\n",
    "        # for tag, value in model.named_parameters():\n",
    "        #     tag = tag.replace('.', '/')\n",
    "        #     board_logger.histo_summary(tag, value.data.cpu().numpy(), epoch+1)\n",
    "        #     # board_logger.histo_summary(tag+'/grad', value.grad.data.cpu().numpy(), epoch+1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea7e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% AlexNet Module\n",
    "\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=(5, 5), padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def alexnet(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet(**kwargs)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8377c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% privacy_train\n",
    "def privacy_train(trainloader, model, inference_model, criterion, optimizer, use_cuda, num_batchs=1000):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    mtop1_a = AverageMeter()\n",
    "    mtop5_a = AverageMeter()\n",
    "\n",
    "    inference_model.train()\n",
    "    model.eval()\n",
    "    # switch to evaluate mode\n",
    "\n",
    "    end = time.time()\n",
    "    first_id = -1\n",
    "    for batch_idx, ((tr_input, tr_target), (te_input, te_target)) in trainloader:\n",
    "        # measure data loading time\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            tr_input = tr_input.cuda()\n",
    "            te_input = te_input.cuda()\n",
    "            tr_target = tr_target.cuda()\n",
    "            te_target = te_target.cuda()\n",
    "\n",
    "        v_tr_input = torch.autograd.Variable(tr_input)\n",
    "        v_te_input = torch.autograd.Variable(te_input)\n",
    "        v_tr_target = torch.autograd.Variable(tr_target)\n",
    "        v_te_target = torch.autograd.Variable(te_target)\n",
    "\n",
    "        # compute output\n",
    "        model_input = torch.cat((v_tr_input, v_te_input))\n",
    "\n",
    "        pred_outputs = model(model_input)\n",
    "        #y_hat\n",
    "\n",
    "        infer_input = torch.cat((v_tr_target, v_te_target))\n",
    "        #(y_hat)\n",
    "\n",
    "        # TODO fix\n",
    "        # mtop1, mtop5 = accuracy(pred_outputs.data, infer_input.data, topk=(1, 5))\n",
    "        mtop1 = top_k_accuracy_score(y_true=infer_input.data.cpu(), y_score=pred_outputs.data.cpu(),\n",
    "                                     k=1, labels=range(num_classes))\n",
    "\n",
    "        mtop5 = top_k_accuracy_score(y_true=infer_input.data.cpu(), y_score=pred_outputs.data.cpu(),\n",
    "                                     k=5, labels=range(num_classes))\n",
    "\n",
    "        mtop1_a.update(mtop1, model_input.size(0))\n",
    "        mtop5_a.update(mtop5, model_input.size(0))\n",
    "\n",
    "        one_hot_tr = torch.from_numpy((np.zeros((infer_input.size(0), num_classes)) - 1)).cuda().type(torch.float)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, infer_input.type(torch.int64).view([-1, 1]).data, 1)\n",
    "\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        #ONE_hot y_hat\n",
    "\n",
    "        attack_model_input = pred_outputs  # torch.cat((pred_outputs,infer_input_one_hot),1)\n",
    "        member_output = inference_model(attack_model_input, infer_input_one_hot)\n",
    "        #inf_model(y,y_hat)\n",
    "        #member->?0/1\n",
    "\n",
    "        is_member_labels = torch.from_numpy(\n",
    "            np.reshape(\n",
    "                np.concatenate((np.zeros(v_tr_input.size(0)), np.ones(v_te_input.size(0)))),\n",
    "                [-1, 1]\n",
    "            )\n",
    "        ).cuda()\n",
    "\n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels).type(torch.float)\n",
    "        #true_labels\n",
    "\n",
    "        loss = criterion(member_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = np.mean((member_output.data.cpu().numpy() > 0.5) == v_is_member_labels.data.cpu().numpy())\n",
    "        losses.update(loss.data.item(), model_input.size(0))\n",
    "        top1.update(prec1, model_input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx - first_id > num_batchs:\n",
    "            break\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(report_str(batch_idx, data_time.avg, batch_time.avg, losses.avg, top1.avg, None))\n",
    "\n",
    "    return losses.avg, top1.avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1017e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"resnext in pytorch\n",
    "[1] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He.\n",
    "    Aggregated Residual Transformations for Deep Neural Networks\n",
    "    https://arxiv.org/abs/1611.05431\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#only implements ResNext bottleneck c\n",
    "\n",
    "\n",
    "#\"\"\"This strategy exposes a new dimension, which we call “cardinality”\n",
    "#(the size of the set of transformations), as an essential factor\n",
    "#in addition to the dimensions of depth and width.\"\"\"\n",
    "CARDINALITY = 32\n",
    "DEPTH = 4\n",
    "BASEWIDTH = 64\n",
    "\n",
    "#\"\"\"The grouped convolutional layer in Fig. 3(c) performs 32 groups\n",
    "#of convolutions whose input and output channels are 4-dimensional.\n",
    "#The grouped convolutional layer concatenates them as the outputs\n",
    "#of the layer.\"\"\"\n",
    "\n",
    "class ResNextBottleNeckC(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super().__init__()\n",
    "\n",
    "        C = CARDINALITY #How many groups a feature map was splitted into\n",
    "\n",
    "        #\"\"\"We note that the input/output width of the template is fixed as\n",
    "        #256-d (Fig. 3), We note that the input/output width of the template\n",
    "        #is fixed as 256-d (Fig. 3), and all widths are dou- bled each time\n",
    "        #when the feature map is subsampled (see Table 1).\"\"\"\n",
    "        D = int(DEPTH * out_channels / BASEWIDTH) #number of channels per group\n",
    "        self.split_transforms = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, C * D, kernel_size=1, groups=C, bias=False),\n",
    "            nn.BatchNorm2d(C * D),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(C * D, C * D, kernel_size=3, stride=stride, groups=C, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(C * D),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(C * D, out_channels * 4, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * 4),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * 4:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * 4, stride=stride, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * 4)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.split_transforms(x) + self.shortcut(x))\n",
    "\n",
    "class ResNext(nn.Module):\n",
    "\n",
    "    def __init__(self, block, num_blocks, class_names=10):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.conv2 = self._make_layer(block, num_blocks[0], 64, 1)\n",
    "        self.conv3 = self._make_layer(block, num_blocks[1], 128, 2)\n",
    "        self.conv4 = self._make_layer(block, num_blocks[2], 256, 2)\n",
    "        self.conv5 = self._make_layer(block, num_blocks[3], 512, 2)\n",
    "        self.avg = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(512 * 4, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.avg(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x= self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def _make_layer(self, block, num_block, out_channels, stride):\n",
    "        \"\"\"Building resnext block\n",
    "        Args:\n",
    "            block: block type(default resnext bottleneck c)\n",
    "            num_block: number of blocks per layer\n",
    "            out_channels: output channels per block\n",
    "            stride: block stride\n",
    "        Returns:\n",
    "            a resnext layer\n",
    "        \"\"\"\n",
    "        strides = [stride] + [1] * (num_block - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * 4\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "def resnext50():\n",
    "    \"\"\" return a resnext50(c32x4d) network\n",
    "    \"\"\"\n",
    "    return ResNext(ResNextBottleNeckC, [3, 4, 6, 3])\n",
    "\n",
    "def resnext101():\n",
    "    \"\"\" return a resnext101(c32x4d) network\n",
    "    \"\"\"\n",
    "    return ResNext(ResNextBottleNeckC, [3, 4, 23, 3])\n",
    "\n",
    "def resnext152():\n",
    "    \"\"\" return a resnext101(c32x4d) network\n",
    "    \"\"\"\n",
    "    return ResNext(ResNextBottleNeckC, [3, 4, 36, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd7a373",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes, grayscale):\n",
    "        self.inplanes = 64\n",
    "        if grayscale:\n",
    "            in_dim = 1\n",
    "        else:\n",
    "            in_dim = 3\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_dim, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1, padding=2)\n",
    "        #self.fc = nn.Linear(2048 * block.expansion, num_classes)\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, (2. / n)**.5)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        #x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc(x)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas\n",
    "\n",
    "\n",
    "\n",
    "def resnet101(num_classes, grayscale):\n",
    "    \"\"\"Constructs a ResNet-101 model.\"\"\"\n",
    "    model = ResNet(block=Bottleneck, \n",
    "                   layers=[3, 4, 23, 3],\n",
    "                   num_classes=10,\n",
    "                   grayscale=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969b9e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' ConvNet '''\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, channel, num_classes, net_width, net_depth, net_act, net_norm, net_pooling, im_size = (32,32)):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.features, shape_feat = self._make_layers(channel, net_width, net_depth, net_norm, net_act, net_pooling, im_size)\n",
    "        num_feat = shape_feat[0]*shape_feat[1]*shape_feat[2]\n",
    "        self.classifier = nn.Linear(num_feat, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"MODEL DATA ON: \", x.get_device(), \"MODEL PARAMS ON: \", self.classifier.weight.data.get_device())\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _get_activation(self, net_act):\n",
    "        if net_act == 'sigmoid':\n",
    "            return nn.Sigmoid()\n",
    "        elif net_act == 'relu':\n",
    "            return nn.ReLU(inplace=True)\n",
    "        elif net_act == 'leakyrelu':\n",
    "            return nn.LeakyReLU(negative_slope=0.01)\n",
    "        else:\n",
    "            exit('unknown activation function: %s'%net_act)\n",
    "\n",
    "    def _get_pooling(self, net_pooling):\n",
    "        if net_pooling == 'maxpooling':\n",
    "            return nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        elif net_pooling == 'avgpooling':\n",
    "            return nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        elif net_pooling == 'none':\n",
    "            return None\n",
    "        else:\n",
    "            exit('unknown net_pooling: %s'%net_pooling)\n",
    "\n",
    "    def _get_normlayer(self, net_norm, shape_feat):\n",
    "        # shape_feat = (c*h*w)\n",
    "        if net_norm == 'batchnorm':\n",
    "            return nn.BatchNorm2d(shape_feat[0], affine=True)\n",
    "        elif net_norm == 'layernorm':\n",
    "            return nn.LayerNorm(shape_feat, elementwise_affine=True)\n",
    "        elif net_norm == 'instancenorm':\n",
    "            return nn.GroupNorm(shape_feat[0], shape_feat[0], affine=True)\n",
    "        elif net_norm == 'groupnorm':\n",
    "            return nn.GroupNorm(4, shape_feat[0], affine=True)\n",
    "        elif net_norm == 'none':\n",
    "            return None\n",
    "        else:\n",
    "            exit('unknown net_norm: %s'%net_norm)\n",
    "\n",
    "    def _make_layers(self, channel, net_width, net_depth, net_norm, net_act, net_pooling, im_size):\n",
    "        layers = []\n",
    "        in_channels = channel\n",
    "        if im_size[0] == 28:\n",
    "            im_size = (32, 32)\n",
    "        shape_feat = [in_channels, im_size[0], im_size[1]]\n",
    "        for d in range(net_depth):\n",
    "            layers += [nn.Conv2d(in_channels, net_width, kernel_size=3, padding=3 if channel == 1 and d == 0 else 1)]\n",
    "            shape_feat[0] = net_width\n",
    "            if net_norm != 'none':\n",
    "                layers += [self._get_normlayer(net_norm, shape_feat)]\n",
    "            layers += [self._get_activation(net_act)]\n",
    "            in_channels = net_width\n",
    "            if net_pooling != 'none':\n",
    "                layers += [self._get_pooling(net_pooling)]\n",
    "                shape_feat[1] //= 2\n",
    "                shape_feat[2] //= 2\n",
    "\n",
    "\n",
    "        return nn.Sequential(*layers), shape_feat \n",
    "    \n",
    "    \n",
    "\n",
    "def get_default_convnet_setting():\n",
    "    net_width, net_depth, net_act, net_norm, net_pooling = 128, 3, 'relu', 'instancenorm', 'avgpooling'\n",
    "    return net_width, net_depth, net_act, net_norm, net_pooling\n",
    "\n",
    "net_width, net_depth, net_act, net_norm, net_pooling = get_default_convnet_setting()\n",
    "nets= ConvNet(channel=3, num_classes= 10,net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling,im_size=(32,32))\n",
    "nets2= ConvNet(channel=3, num_classes= 10,net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling,im_size=(32,32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eac3db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import model.net as net\n",
    "\n",
    "# use GPU if available\n",
    "params.cuda = torch.cuda.is_available()\n",
    "\n",
    "# Set the random seed for reproducible experiments\n",
    "# random.seed(230)\n",
    "# torch.manual_seed(230)\n",
    "# if params.cuda: torch.cuda.manual_seed(230)\n",
    "\n",
    "# Set the logger\n",
    "utilse.set_logger(os.path.join(args.model_dir, 'train.log'))\n",
    "\n",
    "# Create the input data pipeline\n",
    "logging.info(\"Loading the datasets...\")\n",
    "\n",
    "# fetch dataloaders, considering full-set vs. sub-set scenarios\n",
    "if params.subset_percent < 1.0:\n",
    "    train_dl = data_loader.fetch_subset_dataloader('train', params)\n",
    "else:\n",
    "    train_dl = data_loader.fetch_dataloader('train', params)\n",
    "    \n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import io\n",
    "\n",
    "x_train=torch.load('D:/research_2022/mtt-distillation-main/mtt-distillation-main/logged_files/CIFAR10/project/images_best.pt',map_location=lambda storage, loc: storage.cuda(0))\n",
    "y_train=torch.load('D:/research_2022/mtt-distillation-main/mtt-distillation-main/logged_files/CIFAR10/project/labels_best.pt',map_location=lambda storage, loc: storage.cuda(0))\n",
    "\n",
    "\n",
    "save_dir='logged_files/'\n",
    "with open('D:/research_2022/mtt-distillation-main/mtt-distillation-main/logged_files/CIFAR10/project/images_best.pt', 'rb') as f:\n",
    "    buffer = io.BytesIO(f.read())\n",
    "    print(buffer)\n",
    "torch.load(buffer)\n",
    "\n",
    "\n",
    "train_x =x_train\n",
    "#train_x = torch.stack([(x) for x in x_gen])\n",
    "train_y = y_train\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "\n",
    "\n",
    "# train_x=train_x[0:60000,:,:,:]\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "full_indices = np.arange(len(train_x))\n",
    "\n",
    "np.random.shuffle(full_indices)\n",
    "tensor_x = train_x[full_indices]\n",
    "tensor_y = train_y[full_indices]\n",
    "\n",
    "full_indices = np.arange(len(train_y))\n",
    "np.random.shuffle(full_indices)\n",
    "tensor_x = train_x[full_indices]\n",
    "tensor_y = train_y[full_indices]\n",
    "\n",
    "trainset = data.TensorDataset(tensor_x, tensor_y)  # create your datset\n",
    "train_dl = data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "dev_dl = data_loader.fetch_dataloader('dev', params)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "logging.info(\"- done.\")\n",
    "\n",
    "\"\"\"Based on the model_version, determine model/optimizer and KD training mode\n",
    "   WideResNet and DenseNet were trained on multi-GPU; need to specify a dummy\n",
    "   nn.DataParallel module to correctly load the model parameters\n",
    "\"\"\"\n",
    "if \"distill\" in params.model_version:\n",
    "\n",
    "    # train a 5-layer CNN or a 18-layer ResNet with knowledge distillation\n",
    "    if params.model_version == \"cnn_distill\":\n",
    "        print(\"KD with CNN\")\n",
    "        #model=alexnets.AlexNet().cuda() if params.cuda else alexnets.AlexNeT(params)\n",
    "        #model=densenet.DenseNet(num_classes=10).cuda()\n",
    "        #model=resnext50().cuda() if params.cuda else resnext50()\n",
    "        model=ConvNet(channel=3, num_classes= 10,net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling,im_size=(32,32)).cuda()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params.learning_rate*5)\n",
    "        student_checkpoint = 'net_conv_0.64964cc_test_mean.pth'\n",
    "        #model.load_state_dict(torch.load(student_checkpoint))\n",
    "        #model = resnet101().cuda()\n",
    "        \n",
    "        # fetch loss function and metrics definition in model files\n",
    "        loss_fn_kd = net.loss_fn_kd\n",
    "        metrics = net.metrics\n",
    "\n",
    "    elif params.model_version == 'resnet18_distill':\n",
    "        model = resnet.ResNet18().cuda() if params.cuda else resnet.ResNet18()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=params.learning_rate,\n",
    "                              momentum=0.9, weight_decay=5e-4)\n",
    "        # fetch loss function and metrics definition in model files\n",
    "        loss_fn_kd =alexnets.loss_fn_kd\n",
    "        metrics = resnet.metrics\n",
    "\n",
    "    \"\"\" \n",
    "        Specify the pre-trained teacher models for knowledge distillation\n",
    "        Important note: wrn/densenet/resnext/preresnet were pre-trained models using multi-GPU,\n",
    "        therefore need to call \"nn.DaraParallel\" to correctly load the model weights\n",
    "        Trying to run on CPU will then trigger errors (too time-consuming anyway)!\n",
    "    \"\"\"\n",
    "    if params.teacher == \"resnet18\":\n",
    "        print(\"ALEX teacher\")\n",
    "        #teacher_model = alexnets.AlexNet()\n",
    "        teacher_model = resnext50()\n",
    "        #model=nets.cuda()\n",
    "        #teacher_model=nets2\n",
    "        #print(teacher_model)\n",
    "        teacher_checkpoint = 'D:/research_2022/PyTorch-GAN-master/PyTorch-GAN-master/implementations/cgan/data/model/TeacherR50.pth'\n",
    "        teacher_checkpoint = 'D:/research_2022/PyTorch-GAN-master/PyTorch-GAN-master/implementations/cgan/data/model/TeacherRES_DD.pth'\n",
    "        #teacher_model = densenet.DenseNet(num_classes=10)\n",
    "        #teacher_model = densenet.DenseNet(num_classes=10)\n",
    "        #teacher_model = resnet.ResNet18().cuda()\n",
    "        #teacher_checkpoint = 'experiments/base_cnn/epoch399'\n",
    "        teacher_model = teacher_model.cuda()\n",
    "        teacher_model.load_state_dict(torch.load(teacher_checkpoint))\n",
    "        teacher_model = teacher_model.cuda()\n",
    "        \n",
    "    elif params.teacher == \"wrn\":\n",
    "        teacher_model = wrn.WideResNet(depth=28, num_classes=10, widen_factor=10,\n",
    "                                       dropRate=0.3)\n",
    "        teacher_checkpoint = 'experiments/base_wrn/best.pth.tar'\n",
    "        teacher_model = nn.DataParallel(teacher_model).cuda()\n",
    "\n",
    "    elif params.teacher == \"densenet\":\n",
    "        teacher_model = densenet.DenseNet(depth=100, growthRate=12)\n",
    "        teacher_checkpoint = 'experiments/base_densenet/best.pth.tar'\n",
    "        teacher_model = nn.DataParallel(teacher_model).cuda()\n",
    "\n",
    "    elif params.teacher == \"resnext29\":\n",
    "#         teacher_model = resnext.CifarResNeXt(cardinality=8, depth=29, num_classes=10)\n",
    "#         teacher_checkpoint = 'experiments/base_resnext29/best.pth.tar'\n",
    "#         teacher_model = nn.DataParallel(teacher_model).cuda()\n",
    "#         teacher_model = alexnets.AlexNet()\n",
    "        teacher_model = densenet.DenseNet(num_classes=10)\n",
    "        teacher_checkpoint = 'experiments/base_cnn/epoch399'\n",
    "        teacher_model = teacher_model.cuda()\n",
    "\n",
    "    elif params.teacher == \"preresnet110\":\n",
    "        teacher_model = preresnet.PreResNet(depth=110, num_classes=10)\n",
    "        teacher_checkpoint = 'experiments/base_preresnet110/best.pth.tar'\n",
    "        teacher_model = nn.DataParallel(teacher_model).cuda()\n",
    "        \n",
    "    elif params.teacher == \"AlexNet\":\n",
    "        teacher_model = AlexNet(num_classes)\n",
    "        teacher_checkpoint = 'F:\\Other\\privacy_reg\\results\\epoch399.pth.tar'\n",
    "        teacher_model = nn.DataParallel(teacher_model).cuda()\n",
    "        \n",
    "    #teacher_checkpoint = 'D:/research_2022/PyTorch-GAN-master/PyTorch-GAN-master/implementations/cgan/data/model/TeacherR50.pth'\n",
    "    #utilse.load_checkpoint(teacher_checkpoint, teacher_model)\n",
    "    \n",
    "\n",
    "    # Train the model with KD\n",
    "    logging.info(\"Experiment - model version: {}\".format(params.model_version))\n",
    "    logging.info(\"Starting training for {} epoch(s)\".format(params.num_epochs))\n",
    "    logging.info(\"First, loading the teacher model and computing its outputs...\")\n",
    "    \n",
    "    train_and_evaluate_kd(model, teacher_model, train_dl, dev_dl, optimizer, loss_fn_kd,\n",
    "                          metrics, params, args.model_dir, args.restore_file)\n",
    "#     loss_fn=alexnets.loss_fn\n",
    "#     train_and_evaluate(model, train_dl, dev_dl, optimizer, loss_fn, metrics, params,\n",
    "#                        args.model_dir, args.restore_file)\n",
    "else:\n",
    "    if params.model_version == \"cnn\":\n",
    "        model = net.Net(params).cuda() if params.cuda else net.Net(params)\n",
    "        #optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "        # fetch loss function and metrics\n",
    "        loss_fn = net.loss_fn\n",
    "        metrics = net.metrics\n",
    "\n",
    "    elif params.model_version == \"resnet18\":\n",
    "        model = resnet.ResNet18().cuda() if params.cuda else resnet.ResNet18()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=params.learning_rate,\n",
    "                              momentum=0.9, weight_decay=5e-4)\n",
    "        # fetch loss function and metrics\n",
    "        loss_fn = resnet.loss_fn\n",
    "        metrics = resnet.metrics\n",
    "\n",
    "\n",
    "\n",
    "    # elif params.model_version == \"wrn\":\n",
    "    #     model = wrn.wrn(depth=28, num_classes=10, widen_factor=10, dropRate=0.3)\n",
    "    #     model = model.cuda() if params.cuda else model\n",
    "    #     optimizer = optim.SGD(model.parameters(), lr=params.learning_rate,\n",
    "    #                           momentum=0.9, weight_decay=5e-4)\n",
    "    #     # fetch loss function and metrics\n",
    "    #     loss_fn = wrn.loss_fn\n",
    "    #     metrics = wrn.metrics\n",
    "        \n",
    "    # Train the model\n",
    "    logging.info(\"Starting training for {} epoch(s)\".format(params.num_epochs))\n",
    "    train_and_evaluate(model, train_dl, dev_dl, optimizer, loss_fn, metrics, params,\n",
    "                       args.model_dir, args.restore_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410d11f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Simplified version of DLA in PyTorch.\n",
    "\n",
    "Note this implementation is not identical to the original paper version.\n",
    "But it seems works fine.\n",
    "\n",
    "See dla.py for the original paper version.\n",
    "\n",
    "Reference:\n",
    "    Deep Layer Aggregation. https://arxiv.org/abs/1707.06484\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Root(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1):\n",
    "        super(Root, self).__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size,\n",
    "            stride=1, padding=(kernel_size - 1) // 2, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        x = torch.cat(xs, 1)\n",
    "        out = F.relu(self.bn(self.conv(x)))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Tree(nn.Module):\n",
    "    def __init__(self, block, in_channels, out_channels, level=1, stride=1):\n",
    "        super(Tree, self).__init__()\n",
    "        self.root = Root(2*out_channels, out_channels)\n",
    "        if level == 1:\n",
    "            self.left_tree = block(in_channels, out_channels, stride=stride)\n",
    "            self.right_tree = block(out_channels, out_channels, stride=1)\n",
    "        else:\n",
    "            self.left_tree = Tree(block, in_channels,\n",
    "                                  out_channels, level=level-1, stride=stride)\n",
    "            self.right_tree = Tree(block, out_channels,\n",
    "                                   out_channels, level=level-1, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.left_tree(x)\n",
    "        out2 = self.right_tree(out1)\n",
    "        out = self.root([out1, out2])\n",
    "        return out\n",
    "\n",
    "\n",
    "class SimpleDLA(nn.Module):\n",
    "    def __init__(self, block=BasicBlock, num_classes=10):\n",
    "        super(SimpleDLA, self).__init__()\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.layer3 = Tree(block,  32,  64, level=1, stride=1)\n",
    "        self.layer4 = Tree(block,  64, 128, level=2, stride=2)\n",
    "        self.layer5 = Tree(block, 128, 256, level=2, stride=2)\n",
    "        self.layer6 = Tree(block, 256, 512, level=1, stride=2)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.base(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = SimpleDLA()\n",
    "    print(net)\n",
    "    x = torch.randn(1, 3, 32, 32)\n",
    "    y = net(x)\n",
    "    print(y.size())\n",
    "    \n",
    "def load_checkpoint(checkpoint, model, optimizer=None):\n",
    "    \"\"\"Loads model parameters (state_dict) from file_path. If optimizer is provided, loads state_dict of\n",
    "    optimizer assuming it is present in checkpoint.\n",
    "\n",
    "    Args:\n",
    "        checkpoint: (string) filename which needs to be loaded\n",
    "        model: (torch.nn.Module) model for which the parameters are loaded\n",
    "        optimizer: (torch.optim) optional: resume optimizer from checkpoint\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint):\n",
    "        raise(\"File doesn't exist {}\".format(checkpoint))\n",
    "    if torch.cuda.is_available():\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "    else:\n",
    "        # this helps avoid errors when loading single-GPU-trained weights onto CPU-model\n",
    "        checkpoint = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
    "\n",
    "    model.load_state_dict(checkpoint['net'])\n",
    "\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "    return checkpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1244353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import model.net as net\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# use GPU if available\n",
    "params.cuda = torch.cuda.is_available()\n",
    "\n",
    "# Set the random seed for reproducible experiments\n",
    "# random.seed(230)\n",
    "# torch.manual_seed(230)\n",
    "# if params.cuda: torch.cuda.manual_seed(230)\n",
    "\n",
    "# Set the logger\n",
    "utilse.set_logger(os.path.join(args.model_dir, 'train.log'))\n",
    "\n",
    "# Create the input data pipeline\n",
    "logging.info(\"Loading the datasets...\")\n",
    "\n",
    "# fetch dataloaders, considering full-set vs. sub-set scenarios\n",
    "if params.subset_percent < 1.0:\n",
    "    train_dl = data_loader.fetch_subset_dataloader('train', params)\n",
    "else:\n",
    "    train_dl = data_loader.fetch_dataloader('train', params)\n",
    "    \n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import io\n",
    "\n",
    "x_train=torch.load('D:/research_2022/mtt-distillation-main/mtt-distillation-main/logged_files/CIFAR10/project/images_best.pt',map_location=lambda storage, loc: storage.cuda(0))\n",
    "y_train=torch.load('D:/research_2022/mtt-distillation-main/mtt-distillation-main/logged_files/CIFAR10/project/labels_best.pt',map_location=lambda storage, loc: storage.cuda(0))\n",
    "\n",
    "\n",
    "save_dir='logged_files/'\n",
    "with open('D:/research_2022/mtt-distillation-main/mtt-distillation-main/logged_files/CIFAR10/project/images_best.pt', 'rb') as f:\n",
    "    buffer = io.BytesIO(f.read())\n",
    "    print(buffer)\n",
    "torch.load(buffer)\n",
    "\n",
    "\n",
    "train_x =x_train\n",
    "#train_x = torch.stack([(x) for x in x_gen])\n",
    "train_y = y_train\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "\n",
    "\n",
    "# train_x=train_x[0:60000,:,:,:]\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "full_indices = np.arange(len(train_x))\n",
    "\n",
    "np.random.shuffle(full_indices)\n",
    "tensor_x = train_x[full_indices]\n",
    "tensor_y = train_y[full_indices]\n",
    "\n",
    "full_indices = np.arange(len(train_y))\n",
    "np.random.shuffle(full_indices)\n",
    "tensor_x = train_x[full_indices]\n",
    "tensor_y = train_y[full_indices]\n",
    "\n",
    "trainset = data.TensorDataset(tensor_x, tensor_y)  # create your datset\n",
    "train_dl = data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "dev_dl = data_loader.fetch_dataloader('dev', params)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "logging.info(\"- done.\")\n",
    "\n",
    "\"\"\"Based on the model_version, determine model/optimizer and KD training mode\n",
    "   WideResNet and DenseNet were trained on multi-GPU; need to specify a dummy\n",
    "   nn.DataParallel module to correctly load the model parameters\n",
    "\"\"\"\n",
    "if \"distill\" in params.model_version:\n",
    "\n",
    "    # train a 5-layer CNN or a 18-layer ResNet with knowledge distillation\n",
    "    if params.model_version == \"cnn_distill\":\n",
    "        print(\"KD with CNN\")\n",
    "        #model=alexnets.AlexNet().cuda() if params.cuda else alexnets.AlexNeT(params)\n",
    "        #model=densenet.DenseNet(num_classes=10).cuda()\n",
    "        #model=resnext50().cuda() if params.cuda else resnext50()\n",
    "        model=nets.cuda()\n",
    "        #optimizer = optim.Adam(model.parameters(), lr=params.learning_rate/5)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.06,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.00001,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "        #optimizer = optim.Adam(model.parameters(), lr=params.learning_rate/20)\n",
    "        student_checkpoint = 'net_conv_0.64964cc_test_mean.pth'\n",
    "        #model.load_state_dict(torch.load(student_checkpoint))\n",
    "        #model = resnet101().cuda()\n",
    "        \n",
    "        # fetch loss function and metrics definition in model files\n",
    "        loss_fn_kd = net.loss_fn_kd\n",
    "        metrics = net.metrics\n",
    "\n",
    "    elif params.model_version == 'resnet18_distill':\n",
    "        model = resnet.ResNet18().cuda() if params.cuda else resnet.ResNet18()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=params.learning_rate,\n",
    "                              momentum=0.9, weight_decay=5e-4)\n",
    "        # fetch loss function and metrics definition in model files\n",
    "        loss_fn_kd =alexnets.loss_fn_kd\n",
    "        metrics = resnet.metrics\n",
    "\n",
    "    \"\"\" \n",
    "        Specify the pre-trained teacher models for knowledge distillation\n",
    "        Important note: wrn/densenet/resnext/preresnet were pre-trained models using multi-GPU,\n",
    "        therefore need to call \"nn.DaraParallel\" to correctly load the model weights\n",
    "        Trying to run on CPU will then trigger errors (too time-consuming anyway)!\n",
    "    \"\"\"\n",
    "    if params.teacher == \"resnet18\":\n",
    "        print(\"ALEX teacher\")\n",
    "        #teacher_model = alexnets.AlexNet()\n",
    "        #teacher_model = resnext50()\n",
    "        teacher_model=SimpleDLA()\n",
    "        \n",
    "        #teacher_model=nets2\n",
    "        #print(teacher_model)\n",
    "        teacher_checkpoint = 'D:/research_2022/PyTorch-GAN-master/PyTorch-GAN-master/implementations/cgan/data/model/TeacherR50.pth'\n",
    "        #teacher_checkpoint = 'D:/research_2022/PyTorch-GAN-master/PyTorch-GAN-master/implementations/cgan/data/model/TeacherRES_DD.pth'\n",
    "        #teacher_checkpoint = 'D:/research_2022/PyTorch-GAN-master/PyTorch-GAN-master/implementations/cgan/data/model/TeacherRES_DD101.pth'\n",
    "        #teacher_model = densenet.DenseNet(num_classes=10)\n",
    "        #teacher_model = densenet.DenseNet(num_classes=10)\n",
    "        #teacher_model = resnet.ResNet18().cuda()\n",
    "        #teacher_checkpoint = 'experiments/base_cnn/epoch399'\n",
    "        teacher_checkpoint = 'D:/research_2022/pytorch-cifar-master/pytorch-cifar-master/checkpoint/Teacher_DD.pth'\n",
    "        teacher_model = torch.nn.DataParallel(teacher_model)\n",
    "        cudnn.benchmark = True\n",
    "        teacher_model = teacher_model.cuda()\n",
    "        teacher_model.load_state_dict(torch.load(teacher_checkpoint))\n",
    "        #teacher_model = teacher_model.cuda()\n",
    "        \n",
    "    elif params.teacher == \"wrn\":\n",
    "        teacher_model = wrn.WideResNet(depth=28, num_classes=10, widen_factor=10,\n",
    "                                       dropRate=0.3)\n",
    "        teacher_checkpoint = 'experiments/base_wrn/best.pth.tar'\n",
    "        teacher_model = nn.DataParallel(teacher_model).cuda()\n",
    "\n",
    "    elif params.teacher == \"densenet\":\n",
    "        teacher_model = densenet.DenseNet(depth=100, growthRate=12)\n",
    "        teacher_checkpoint = 'experiments/base_densenet/best.pth.tar'\n",
    "        teacher_model = nn.DataParallel(teacher_model).cuda()\n",
    "\n",
    "    elif params.teacher == \"resnext29\":\n",
    "#         teacher_model = resnext.CifarResNeXt(cardinality=8, depth=29, num_classes=10)\n",
    "#         teacher_checkpoint = 'experiments/base_resnext29/best.pth.tar'\n",
    "#         teacher_model = nn.DataParallel(teacher_model).cuda()\n",
    "#         teacher_model = alexnets.AlexNet()\n",
    "        teacher_model = densenet.DenseNet(num_classes=10)\n",
    "        teacher_checkpoint = 'experiments/base_cnn/epoch399'\n",
    "        teacher_model = teacher_model.cuda()\n",
    "\n",
    "    elif params.teacher == \"preresnet110\":\n",
    "        teacher_model = preresnet.PreResNet(depth=110, num_classes=10)\n",
    "        teacher_checkpoint = 'experiments/base_preresnet110/best.pth.tar'\n",
    "        teacher_model = nn.DataParallel(teacher_model).cuda()\n",
    "        \n",
    "    elif params.teacher == \"AlexNet\":\n",
    "        teacher_model = AlexNet(num_classes)\n",
    "        teacher_checkpoint = 'F:\\Other\\privacy_reg\\results\\epoch399.pth.tar'\n",
    "        teacher_model = nn.DataParallel(teacher_model).cuda()\n",
    "        \n",
    "    #teacher_checkpoint = 'D:/research_2022/PyTorch-GAN-master/PyTorch-GAN-master/implementations/cgan/data/model/TeacherR50.pth'\n",
    "    #utilse.load_checkpoint(teacher_checkpoint, teacher_model)\n",
    "    \n",
    "\n",
    "    # Train the model with KD\n",
    "    logging.info(\"Experiment - model version: {}\".format(params.model_version))\n",
    "    logging.info(\"Starting training for {} epoch(s)\".format(params.num_epochs))\n",
    "    logging.info(\"First, loading the teacher model and computing its outputs...\")\n",
    "    \n",
    "    train_and_evaluate_kd(model, teacher_model, train_dl, dev_dl, optimizer, loss_fn_kd,\n",
    "                          metrics, params, args.model_dir, args.restore_file)\n",
    "#     loss_fn=alexnets.loss_fn\n",
    "#     train_and_evaluate(model, train_dl, dev_dl, optimizer, loss_fn, metrics, params,\n",
    "#                        args.model_dir, args.restore_file)\n",
    "else:\n",
    "    if params.model_version == \"cnn\":\n",
    "        model = net.Net(params).cuda() if params.cuda else net.Net(params)\n",
    "        #optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "        # fetch loss function and metrics\n",
    "        loss_fn = net.loss_fn\n",
    "        metrics = net.metrics\n",
    "\n",
    "    elif params.model_version == \"resnet18\":\n",
    "        model = resnet.ResNet18().cuda() if params.cuda else resnet.ResNet18()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=params.learning_rate,\n",
    "                              momentum=0.9, weight_decay=5e-4)\n",
    "        # fetch loss function and metrics\n",
    "        loss_fn = resnet.loss_fn\n",
    "        metrics = resnet.metrics\n",
    "\n",
    "\n",
    "\n",
    "    # elif params.model_version == \"wrn\":\n",
    "    #     model = wrn.wrn(depth=28, num_classes=10, widen_factor=10, dropRate=0.3)\n",
    "    #     model = model.cuda() if params.cuda else model\n",
    "    #     optimizer = optim.SGD(model.parameters(), lr=params.learning_rate,\n",
    "    #                           momentum=0.9, weight_decay=5e-4)\n",
    "    #     # fetch loss function and metrics\n",
    "    #     loss_fn = wrn.loss_fn\n",
    "    #     metrics = wrn.metrics\n",
    "        \n",
    "    # Train the model\n",
    "    logging.info(\"Starting training for {} epoch(s)\".format(params.num_epochs))\n",
    "    train_and_evaluate(model, train_dl, dev_dl, optimizer, loss_fn, metrics, params,\n",
    "                       args.model_dir, args.restore_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef13767",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use GPU if available\n",
    "params.cuda = torch.cuda.is_available()\n",
    "model = net.Net(params).cuda() if params.cuda else net.Net(params)\n",
    "utilse.load_checkpoint('experiments/base_cnn/best_pth',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3082cf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# teacher_model = alexnet.AlexNet(num_classes=10)\n",
    "# teacher_checkpoint = 'experiments/base_cnn/epoch399'\n",
    "# utilse.load_checkpoint(teacher_checkpoint, teacher_model)\n",
    "\n",
    "# %% Inference Attack HZ Class\n",
    "\n",
    "\n",
    "class InferenceAttack_HZ(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        super(InferenceAttack_HZ, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(self.num_classes, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.labels = nn.Sequential(\n",
    "            nn.Linear(num_classes, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.combine = nn.Sequential(\n",
    "            nn.Linear(64 * 2, 256),\n",
    "\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "        for key in self.state_dict():\n",
    "            print(f'\\t {key}')\n",
    "            if key.split('.')[-1] == 'weight':\n",
    "                nn.init.normal_(self.state_dict()[key], std=0.01)\n",
    "\n",
    "            elif key.split('.')[-1] == 'bias':\n",
    "                self.state_dict()[key][...] = 0\n",
    "\n",
    "        self.output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "\n",
    "        out_x = self.features(x)\n",
    "        out_l = self.labels(labels)\n",
    "\n",
    "        is_member = self.combine(torch.cat((out_x, out_l), 1))\n",
    "\n",
    "        return self.output(is_member)\n",
    "\n",
    "\n",
    "# %% Status Func\n",
    "\n",
    "def report_str(batch_idx, data_time, batch_time, losses, top1, top5):\n",
    "    batch = f'({batch_idx:4d})'\n",
    "    time = f'Data: {data_time:.2f}s | Batch: {batch_time:.2f}s'\n",
    "    loss_ac1 = f'Loss: {losses:.3f} | Top1: {top1 * 100:.2f}%'\n",
    "\n",
    "    res = f'{batch} {time} || {loss_ac1}'\n",
    "\n",
    "    if top5 is None:\n",
    "        return res\n",
    "    else:\n",
    "        return res + f' | Top5: {top5 * 100:.2f}%'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c579ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "# %% privacy_train\n",
    "# torch.Size([128, 3, 32, 32]) torch.Size([128]) torch.Size([128, 3, 32, 32]) torch.Size([128])\n",
    "# PRED torch.Size([256, 10])\n",
    "# infer_in torch.Size([256])\n",
    "def privacy_train(trainloader, model, inference_model, criterion, optimizer, use_cuda, num_batchs):\n",
    "    num_classes=10\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    mtop1_a = AverageMeter()\n",
    "    mtop5_a = AverageMeter()\n",
    "\n",
    "    inference_model.train()\n",
    "    model.eval()\n",
    "    # switch to evaluate mode\n",
    "\n",
    "    end = time.time()\n",
    "    first_id = -1\n",
    "    for batch_idx, ((tr_input, tr_target), (te_input, te_target)) in trainloader:\n",
    "        # measure data loading time\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        #print(tr_input.shape, tr_target.shape,te_input.shape, te_target.shape)\n",
    "\n",
    "        if use_cuda:\n",
    "            tr_input = tr_input.cuda()\n",
    "            te_input = te_input.cuda()\n",
    "            tr_target = tr_target.cuda()\n",
    "            te_target = te_target.cuda()\n",
    "\n",
    "        v_tr_input = torch.autograd.Variable(tr_input)\n",
    "        v_te_input = torch.autograd.Variable(te_input)\n",
    "        v_tr_target = torch.autograd.Variable(tr_target)\n",
    "        v_te_target = torch.autograd.Variable(te_target)\n",
    "\n",
    "        # compute output\n",
    "        model_input = torch.cat((v_tr_input, v_te_input))\n",
    "\n",
    "        pred_outputs = model(model_input)\n",
    "        #print(\"PRED\",pred_outputs.shape)\n",
    "        #y_hat\n",
    "\n",
    "        infer_input = torch.cat((v_tr_target, v_te_target))\n",
    "        #print(\"infer_in\",infer_input.shape)\n",
    "        #(y_hat)\n",
    "\n",
    "        # TODO fix\n",
    "        # mtop1, mtop5 = accuracy(pred_outputs.data, infer_input.data, topk=(1, 5))\n",
    "        mtop1 = top_k_accuracy_score(y_true=infer_input.data.cpu(), y_score=pred_outputs.data.cpu(),\n",
    "                                     k=1, labels=range(num_classes))\n",
    "\n",
    "        mtop5 = top_k_accuracy_score(y_true=infer_input.data.cpu(), y_score=pred_outputs.data.cpu(),\n",
    "                                     k=5, labels=range(num_classes))\n",
    "\n",
    "        mtop1_a.update(mtop1, model_input.size(0))\n",
    "        mtop5_a.update(mtop5, model_input.size(0))\n",
    "\n",
    "        one_hot_tr = torch.from_numpy((np.zeros((infer_input.size(0), num_classes)) - 1)).cuda().type(torch.float)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, infer_input.type(torch.int64).view([-1, 1]).data, 1)\n",
    "\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        #ONE_hot y_hat\n",
    "\n",
    "        attack_model_input = pred_outputs  # torch.cat((pred_outputs,infer_input_one_hot),1)\n",
    "        member_output = inference_model(attack_model_input, infer_input_one_hot)\n",
    "        #inf_model(y,y_hat)\n",
    "        #member->?0/1\n",
    "\n",
    "        is_member_labels = torch.from_numpy(\n",
    "            np.reshape(\n",
    "                np.concatenate((np.zeros(v_tr_input.size(0)), np.ones(v_te_input.size(0)))),\n",
    "                [-1, 1]\n",
    "            )\n",
    "        ).cuda()\n",
    "\n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels).type(torch.float)\n",
    "        #true_labels\n",
    "\n",
    "        loss = criterion(member_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = np.mean((member_output.data.cpu().numpy() > 0.5) == v_is_member_labels.data.cpu().numpy())\n",
    "        losses.update(loss.data.item(), model_input.size(0))\n",
    "        top1.update(prec1, model_input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx - first_id > num_batchs:\n",
    "            break\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx % 50 == 0:\n",
    "            #print(\"STUCK\")\n",
    "            print( losses.avg, top1.avg)\n",
    "            #print(report_str(batch_idx, data_time.avg, batch_time.avg, losses.avg, top1.avg, None))\n",
    "\n",
    "    return losses.avg, top1.avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b33ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "train_mean = np.array([125.307, 122.950, 113.865])\n",
    "train_std = np.array([62.993, 62.089, 66.705])\n",
    "test_mean = np.array([126.025, 123.708, 114.854])\n",
    "test_std = np.array([62.896, 61.937, 66.706])\n",
    "\n",
    "# Normalize mean std to 0..1 from 0..255\n",
    "train_mean /= 255\n",
    "train_std /= 255\n",
    "test_mean /= 255\n",
    "test_std /= 255\n",
    "\n",
    "print(f'Hard code CIFAR10 train/test mean/std for next time')\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(test_mean, test_std),\n",
    "])\n",
    "\n",
    "# TODO check loader for trainloader_private\n",
    "trainset_private =datasets.CIFAR10(root='./data-cifar10', train=True,\n",
    "        download=True, transform=transform_test)\n",
    "trainloader_private = torch.utils.data.DataLoader(trainset_private, batch_size=params.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f781af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% checkpoint, adjust LR\n",
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    if epoch in [20, 40]:\n",
    "        state['lr'] *= 0.1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = state['lr']\n",
    "\n",
    "\n",
    "def save_checkpoint_adversary(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_adversary_best.pth.tar'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5553e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference_model = torch.nn.DataParallel(inferenece_model).cuda()\n",
    "\n",
    "LR = 0.05\n",
    "EPOCHS = 40\n",
    "print('\\tTotal params: %.2fM' % (sum(p.numel() for p in model.parameters()) / 1000000.0))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "criterion_attack = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "inference_model = InferenceAttack_HZ(10).cuda()\n",
    "\n",
    "private_train_criterion = nn.MSELoss()\n",
    "\n",
    "optimizer_mem = optim.Adam(inference_model.parameters(), lr=0.00001)\n",
    "\n",
    "best_acc = 0.0\n",
    "start_epoch = 0\n",
    "\n",
    "# Train and val\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    #adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    print(f'\\nEpoch: [{epoch + 1:d} | {EPOCHS:d}] ')\n",
    "\n",
    "    train_private_enum = enumerate(zip(trainloader_private, dev_dl))\n",
    "    privacy_loss, privacy_acc = privacy_train(train_private_enum, model, inference_model, criterion_attack, optimizer_mem, True, 100)\n",
    "    print(f'Privacy Res: {privacy_acc * 100:.2f}% ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580899c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join('G:/Research_Summer/THESIS/', 'Teacher_Simple_DLA_to_CNN_DD_69.pth')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b156840",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
