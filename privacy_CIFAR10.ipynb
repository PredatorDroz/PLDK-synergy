{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa60124c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fahim\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\fahim\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "C:\\Users\\fahim\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda: True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from sklearn.metrics import top_k_accuracy_score \n",
    "\n",
    "from utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "import numpy as np\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "print(f'Cuda: {use_cuda}')\n",
    "\n",
    "# manualSeed = random.randint(1, 10000)\n",
    "manualSeed = 0\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(manualSeed)\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "\n",
    "if sys.platform.startswith('linux'):\n",
    "    LINUX = True\n",
    "else:\n",
    "    LINUX = False\n",
    "\n",
    "if LINUX:\n",
    "    DATA_ROOT = r\"/home/data/sets/\"\n",
    "    RESULTS_ROOT = r\"/home/results/privacy_reg/\"\n",
    "    LOG_ROOT = r\"/home/logs/privacy_reg/\"\n",
    "else:\n",
    "    DATA_ROOT = r\"F:\\Other\\data\"\n",
    "    RESULTS_ROOT = r\"F:\\Other\\privacy_reg\\results\"\n",
    "    LOG_ROOT = r\"F:\\logs\"\n",
    "# %%\n",
    "NUM_WORKERS = 0  # 0 for debugging as script?\n",
    "\n",
    "# dataset = 'cifar10'\n",
    "dataset = 'cifar100'\n",
    "\n",
    "LR = 0.05\n",
    "EPOCHS = 400\n",
    "\n",
    "train_batch = 100\n",
    "test_batch = 100\n",
    "state = {'lr': LR}\n",
    "\n",
    "checkpoint_path = os.path.join(RESULTS_ROOT, f'checkpoints_{dataset}_alexnetdefense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023c40c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ResNet in PyTorch.\n",
    "\n",
    "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
    "\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=100):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3,4,6,3])\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3,4,6,3])\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3,4,23,3])\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3,8,36,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479d451",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"resnext in pytorch\n",
    "[1] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He.\n",
    "    Aggregated Residual Transformations for Deep Neural Networks\n",
    "    https://arxiv.org/abs/1611.05431\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#only implements ResNext bottleneck c\n",
    "\n",
    "\n",
    "#\"\"\"This strategy exposes a new dimension, which we call “cardinality”\n",
    "#(the size of the set of transformations), as an essential factor\n",
    "#in addition to the dimensions of depth and width.\"\"\"\n",
    "CARDINALITY = 32\n",
    "DEPTH = 4\n",
    "BASEWIDTH = 64\n",
    "\n",
    "#\"\"\"The grouped convolutional layer in Fig. 3(c) performs 32 groups\n",
    "#of convolutions whose input and output channels are 4-dimensional.\n",
    "#The grouped convolutional layer concatenates them as the outputs\n",
    "#of the layer.\"\"\"\n",
    "\n",
    "class ResNextBottleNeckC(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super().__init__()\n",
    "\n",
    "        C = CARDINALITY #How many groups a feature map was splitted into\n",
    "\n",
    "        #\"\"\"We note that the input/output width of the template is fixed as\n",
    "        #256-d (Fig. 3), We note that the input/output width of the template\n",
    "        #is fixed as 256-d (Fig. 3), and all widths are dou- bled each time\n",
    "        #when the feature map is subsampled (see Table 1).\"\"\"\n",
    "        D = int(DEPTH * out_channels / BASEWIDTH) #number of channels per group\n",
    "        self.split_transforms = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, C * D, kernel_size=1, groups=C, bias=False),\n",
    "            nn.BatchNorm2d(C * D),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(C * D, C * D, kernel_size=3, stride=stride, groups=C, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(C * D),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(C * D, out_channels * 4, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * 4),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * 4:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * 4, stride=stride, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * 4)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.split_transforms(x) + self.shortcut(x))\n",
    "\n",
    "class ResNext(nn.Module):\n",
    "\n",
    "    def __init__(self, block, num_blocks, class_names=100):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.conv2 = self._make_layer(block, num_blocks[0], 64, 1)\n",
    "        self.conv3 = self._make_layer(block, num_blocks[1], 128, 2)\n",
    "        self.conv4 = self._make_layer(block, num_blocks[2], 256, 2)\n",
    "        self.conv5 = self._make_layer(block, num_blocks[3], 512, 2)\n",
    "        self.avg = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(512 * 4, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.avg(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x= self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def _make_layer(self, block, num_block, out_channels, stride):\n",
    "        \"\"\"Building resnext block\n",
    "        Args:\n",
    "            block: block type(default resnext bottleneck c)\n",
    "            num_block: number of blocks per layer\n",
    "            out_channels: output channels per block\n",
    "            stride: block stride\n",
    "        Returns:\n",
    "            a resnext layer\n",
    "        \"\"\"\n",
    "        strides = [stride] + [1] * (num_block - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * 4\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "def resnext50():\n",
    "    \"\"\" return a resnext50(c32x4d) network\n",
    "    \"\"\"\n",
    "    return ResNext(ResNextBottleNeckC, [3, 4, 6, 3])\n",
    "\n",
    "def resnext101():\n",
    "    \"\"\" return a resnext101(c32x4d) network\n",
    "    \"\"\"\n",
    "    return ResNext(ResNextBottleNeckC, [3, 4, 23, 3])\n",
    "\n",
    "def resnext152():\n",
    "    \"\"\" return a resnext101(c32x4d) network\n",
    "    \"\"\"\n",
    "    return ResNext(ResNextBottleNeckC, [3, 4, 36, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255ad237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# __all__ = ['densenet']\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, inplanes, expansion=4, growthRate=12, dropRate=0):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        planes = expansion * growthRate\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, growthRate, kernel_size=3, \n",
    "                               padding=1, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropRate = dropRate\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        if self.dropRate > 0:\n",
    "            out = F.dropout(out, p=self.dropRate, training=self.training)\n",
    "\n",
    "        out = torch.cat((x, out), 1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, inplanes, expansion=1, growthRate=12, dropRate=0):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        planes = expansion * growthRate\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.conv1 = nn.Conv2d(inplanes, growthRate, kernel_size=3, \n",
    "                               padding=1, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropRate = dropRate\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        if self.dropRate > 0:\n",
    "            out = F.dropout(out, p=self.dropRate, training=self.training)\n",
    "\n",
    "        out = torch.cat((x, out), 1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, inplanes, outplanes):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.conv1 = nn.Conv2d(inplanes, outplanes, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "#it was 12 (growth rate) now 19\n",
    "    def __init__(self, depth=22, block=Bottleneck, \n",
    "        dropRate=0, num_classes=100, growthRate=19, compressionRate=2):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        assert (depth - 4) % 3 == 0, 'depth should be 3n+4'\n",
    "        n = (depth - 4) / 3 if block == BasicBlock else (depth - 4) // 6\n",
    "\n",
    "        self.growthRate = growthRate\n",
    "        self.dropRate = dropRate\n",
    "\n",
    "        # self.inplanes is a global variable used across multiple\n",
    "        # helper functions\n",
    "        self.inplanes = growthRate * 2 \n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, padding=1,\n",
    "                               bias=False)\n",
    "        self.dense1 = self._make_denseblock(block, n)\n",
    "        self.trans1 = self._make_transition(compressionRate)\n",
    "        self.dense2 = self._make_denseblock(block, n)\n",
    "        self.trans2 = self._make_transition(compressionRate)\n",
    "        self.dense3 = self._make_denseblock(block, n)\n",
    "        self.bn = nn.BatchNorm2d(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.avgpool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(self.inplanes, num_classes)\n",
    "\n",
    "        # Weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_denseblock(self, block, blocks):\n",
    "        layers = []\n",
    "        for i in range(blocks):\n",
    "            # Currently we fix the expansion ratio as the default value\n",
    "            layers.append(block(self.inplanes, growthRate=self.growthRate, dropRate=self.dropRate))\n",
    "            self.inplanes += self.growthRate\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_transition(self, compressionRate):\n",
    "        inplanes = self.inplanes\n",
    "        outplanes = int(math.floor(self.inplanes // compressionRate))\n",
    "        self.inplanes = outplanes\n",
    "        return Transition(inplanes, outplanes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.trans1(self.dense1(x)) \n",
    "        x = self.trans2(self.dense2(x)) \n",
    "        x = self.dense3(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8cbdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% AlexNet Module\n",
    "\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=(5, 5), padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def alexnet(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet(**kwargs)\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc5a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Inference Attack HZ Class\n",
    "\n",
    "\n",
    "class InferenceAttack_HZ(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        super(InferenceAttack_HZ, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(self.num_classes, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.labels = nn.Sequential(\n",
    "            nn.Linear(num_classes, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.combine = nn.Sequential(\n",
    "            nn.Linear(64 * 2, 256),\n",
    "\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "        for key in self.state_dict():\n",
    "            print(f'\\t {key}')\n",
    "            if key.split('.')[-1] == 'weight':\n",
    "                nn.init.normal_(self.state_dict()[key], std=0.01)\n",
    "\n",
    "            elif key.split('.')[-1] == 'bias':\n",
    "                self.state_dict()[key][...] = 0\n",
    "\n",
    "        self.output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "\n",
    "        out_x = self.features(x)\n",
    "        out_l = self.labels(labels)\n",
    "\n",
    "        is_member = self.combine(torch.cat((out_x, out_l), 1))\n",
    "\n",
    "        return self.output(is_member)\n",
    "\n",
    "\n",
    "# %% Status Func\n",
    "\n",
    "def report_str(batch_idx, data_time, batch_time, losses, top1, top5):\n",
    "    batch = f'({batch_idx:4d})'\n",
    "    time = f'Data: {data_time:.2f}s | Batch: {batch_time:.2f}s'\n",
    "    loss_ac1 = f'Loss: {losses:.3f} | Top1: {top1 * 100:.2f}%'\n",
    "\n",
    "    res = f'{batch} {time} || {loss_ac1}'\n",
    "\n",
    "    if top5 is None:\n",
    "        return res\n",
    "    else:\n",
    "        return res + f' | Top5: {top5 * 100:.2f}%'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb9f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% train_privately\n",
    "#alpha=0.9\n",
    "def train_privately(trainloader, model, inference_model, criterion, optimizer, use_cuda, num_batches=10000, alpha=1.9):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    inference_model.eval()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    end = time.time()\n",
    "\n",
    "    first_id = -1\n",
    "    for batch_idx, (inputs, targets) in trainloader:\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda(non_blocking=True)\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        one_hot_tr = torch.from_numpy((np.zeros((outputs.size(0), num_classes)) - 1)).cuda().type(torch.float)\n",
    "\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, targets.type(torch.int64).view([-1, 1]).data, 1)\n",
    "\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "\n",
    "        inference_output = inference_model(outputs, infer_input_one_hot)\n",
    "        # print (inference_output.mean())\n",
    "        loss = criterion(outputs, targets) + alpha * ((inference_output - 1.0).pow(2).mean())\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        # prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "\n",
    "        prec1 = top_k_accuracy_score(y_true=targets.data.cpu(), y_score=outputs.data.cpu(),\n",
    "                                     k=1, labels=range(num_classes))\n",
    "\n",
    "        prec5 = top_k_accuracy_score(y_true=targets.data.cpu(), y_score=outputs.data.cpu(),\n",
    "                                     k=5, labels=range(num_classes))\n",
    "\n",
    "        losses.update(loss.data.item(), inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(report_str(batch_idx + 1, data_time.avg, batch_time.avg, losses.avg, top1.avg, top5.avg))\n",
    "\n",
    "        if batch_idx - first_id >= num_batches:\n",
    "            break\n",
    "\n",
    "    return losses.avg, top1.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129cdc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% train\n",
    "def train(trainloader, model, criterion, optimizer, use_cuda):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    end = time.time()\n",
    "\n",
    "    bar = Bar('Processing', max=len(trainloader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda(non_blocking=True)\n",
    "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        # prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "\n",
    "        prec1 = top_k_accuracy_score(y_true=targets.data.cpu(), y_score=outputs.data.cpu(),\n",
    "                                     k=1, labels=range(num_classes))\n",
    "\n",
    "        prec5 = top_k_accuracy_score(y_true=targets.data.cpu(), y_score=outputs.data.cpu(),\n",
    "                                     k=5, labels=range(num_classes))\n",
    "\n",
    "        losses.update(loss.data.item(), inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(report_str(batch_idx + 1, data_time.avg, batch_time.avg, losses.avg, top1.avg, top5.avg))\n",
    "\n",
    "        return losses.avg, top1.avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aace841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% test\n",
    "def test(testloader, model, criterion, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            # prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "            prec1 = top_k_accuracy_score(y_true=targets.data.cpu(), y_score=outputs.data.cpu(),\n",
    "                                         k=1, labels=range(num_classes))\n",
    "\n",
    "            prec5 = top_k_accuracy_score(y_true=targets.data.cpu(), y_score=outputs.data.cpu(), k=5,\n",
    "                                         labels=range(num_classes))\n",
    "            losses.update(loss.data.item(), inputs.size(0))\n",
    "            top1.update(prec1, inputs.size(0))\n",
    "            top5.update(prec5, inputs.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            # plot progress\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(report_str(batch_idx + 1, data_time.avg, batch_time.avg, losses.avg, top1.avg, top5.avg))\n",
    "\n",
    "    return losses.avg, top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a451fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% privacy_train\n",
    "def privacy_train(trainloader, model, inference_model, criterion, optimizer, use_cuda, num_batchs=1000):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    mtop1_a = AverageMeter()\n",
    "    mtop5_a = AverageMeter()\n",
    "\n",
    "    inference_model.train()\n",
    "    model.eval()\n",
    "    # switch to evaluate mode\n",
    "\n",
    "    end = time.time()\n",
    "    first_id = -1\n",
    "    for batch_idx, ((tr_input, tr_target), (te_input, te_target)) in trainloader:\n",
    "        # measure data loading time\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            tr_input = tr_input.cuda()\n",
    "            te_input = te_input.cuda()\n",
    "            tr_target = tr_target.cuda()\n",
    "            te_target = te_target.cuda()\n",
    "\n",
    "        v_tr_input = torch.autograd.Variable(tr_input)\n",
    "        v_te_input = torch.autograd.Variable(te_input)\n",
    "        v_tr_target = torch.autograd.Variable(tr_target)\n",
    "        v_te_target = torch.autograd.Variable(te_target)\n",
    "\n",
    "        # compute output\n",
    "        model_input = torch.cat((v_tr_input, v_te_input))\n",
    "\n",
    "        pred_outputs = model(model_input)\n",
    "        #y_hat\n",
    "\n",
    "        infer_input = torch.cat((v_tr_target, v_te_target))\n",
    "        #(y_hat)\n",
    "\n",
    "        # TODO fix\n",
    "        # mtop1, mtop5 = accuracy(pred_outputs.data, infer_input.data, topk=(1, 5))\n",
    "        mtop1 = top_k_accuracy_score(y_true=infer_input.data.cpu(), y_score=pred_outputs.data.cpu(),\n",
    "                                     k=1, labels=range(num_classes))\n",
    "\n",
    "        mtop5 = top_k_accuracy_score(y_true=infer_input.data.cpu(), y_score=pred_outputs.data.cpu(),\n",
    "                                     k=5, labels=range(num_classes))\n",
    "\n",
    "        mtop1_a.update(mtop1, model_input.size(0))\n",
    "        mtop5_a.update(mtop5, model_input.size(0))\n",
    "\n",
    "        one_hot_tr = torch.from_numpy((np.zeros((infer_input.size(0), num_classes)) - 1)).cuda().type(torch.float)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, infer_input.type(torch.int64).view([-1, 1]).data, 1)\n",
    "\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        #ONE_hot y_hat\n",
    "\n",
    "        attack_model_input = pred_outputs  # torch.cat((pred_outputs,infer_input_one_hot),1)\n",
    "        member_output = inference_model(attack_model_input, infer_input_one_hot)\n",
    "        #inf_model(y,y_hat)\n",
    "        #member->?0/1\n",
    "\n",
    "        is_member_labels = torch.from_numpy(\n",
    "            np.reshape(\n",
    "                np.concatenate((np.zeros(v_tr_input.size(0)), np.ones(v_te_input.size(0)))),\n",
    "                [-1, 1]\n",
    "            )\n",
    "        ).cuda()\n",
    "\n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels).type(torch.float)\n",
    "        #true_labels\n",
    "\n",
    "        loss = criterion(member_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = np.mean((member_output.data.cpu().numpy() > 0.5) == v_is_member_labels.data.cpu().numpy())\n",
    "        losses.update(loss.data.item(), model_input.size(0))\n",
    "        top1.update(prec1, model_input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx - first_id > num_batchs:\n",
    "            break\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(report_str(batch_idx, data_time.avg, batch_time.avg, losses.avg, top1.avg, None))\n",
    "\n",
    "    return losses.avg, top1.avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7033df7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% privacy_test\n",
    "def privacy_test(trainloader, model, inference_model, criterion, optimizer, use_cuda, num_batchs=1000):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    mtop1_a = AverageMeter()\n",
    "    mtop5_a = AverageMeter()\n",
    "\n",
    "    inference_model.eval()\n",
    "    model.eval()\n",
    "    # switch to evaluate mode\n",
    "\n",
    "    end = time.time()\n",
    "    first_id = -1\n",
    "    for batch_idx, ((tr_input, tr_target), (te_input, te_target)) in trainloader:\n",
    "        # measure data loading time\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            tr_input = tr_input.cuda()\n",
    "            te_input = te_input.cuda()\n",
    "            tr_target = tr_target.cuda()\n",
    "            te_target = te_target.cuda()\n",
    "\n",
    "        v_tr_input = torch.autograd.Variable(tr_input)\n",
    "        v_te_input = torch.autograd.Variable(te_input)\n",
    "        v_tr_target = torch.autograd.Variable(tr_target)\n",
    "        v_te_target = torch.autograd.Variable(te_target)\n",
    "\n",
    "        # compute output\n",
    "        model_input = torch.cat((v_tr_input, v_te_input))\n",
    "\n",
    "        pred_outputs = model(model_input)\n",
    "\n",
    "        infer_input = torch.cat((v_tr_target, v_te_target))\n",
    "\n",
    "        # mtop1, mtop5 = accuracy(pred_outputs.data, infer_input.data, topk=(1, 5))\n",
    "        mtop1 = top_k_accuracy_score(y_true=pred_outputs.data.cpu(), y_score=infer_input.data.cpu(), k=1,\n",
    "                                     labels=range(num_classes))\n",
    "        mtop5 = top_k_accuracy_score(y_true=pred_outputs.data.cpu(), y_score=infer_input.data.cpu(), k=5,\n",
    "                                     labels=range(num_classes))\n",
    "\n",
    "        mtop1_a.update(mtop1, model_input.size(0))\n",
    "        mtop5_a.update(mtop5, model_input.size(0))\n",
    "\n",
    "        one_hot_tr = torch.from_numpy((np.zeros((infer_input.size(0), num_classes)) - 1)).cuda().type(torch.float)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, infer_input.type(torch.int64).view([-1, 1]).data, 1)\n",
    "\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "\n",
    "        attack_model_input = pred_outputs  # torch.cat((pred_outputs,infer_input_one_hot),1)\n",
    "        member_output = inference_model(attack_model_input, infer_input_one_hot)\n",
    "\n",
    "        is_member_labels = torch.from_numpy(\n",
    "            np.reshape(\n",
    "                np.concatenate((np.zeros(v_tr_input.size(0)), np.ones(v_te_input.size(0)))),\n",
    "                [-1, 1]\n",
    "            )\n",
    "        ).cuda()\n",
    "\n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels).type(torch.float)\n",
    "\n",
    "        loss = criterion(member_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = np.mean((member_output.data.cpu().numpy() > 0.5) == v_is_member_labels.data.cpu().numpy())\n",
    "        losses.update(loss.data.item(), model_input.size(0))\n",
    "        top1.update(prec1, model_input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx - first_id >= num_batchs:\n",
    "            break\n",
    "\n",
    "        # plot progress\n",
    "        # if batch_idx%10==0:\n",
    "        #     print(report_str(batch_idx + 1, data_time.avg, batch_time.avg, losses.avg, top1.avg, None))\n",
    "        #     print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "        #             batch=batch_idx ,\n",
    "        #             size=len(trainloader),\n",
    "        #             data=data_time.avg,\n",
    "        #             bt=batch_time.avg,\n",
    "        #             loss=losses.avg,\n",
    "        #             top1=top1.avg,\n",
    "        #             ))\n",
    "\n",
    "    return losses.avg, top1.avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4490711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% checkpoint, adjust LR\n",
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    if epoch in [20, 40]:\n",
    "        state['lr'] *= 0.1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = state['lr']\n",
    "\n",
    "\n",
    "def save_checkpoint_adversary(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_adversary_best.pth.tar'))\n",
    "\n",
    "\n",
    "# %% Dataset\n",
    "dataset ='cifar10'\n",
    "print('==> Preparing dataset %s' % dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5822b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f50457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model.resnet as resnet\n",
    "import model.resnext as resnext\n",
    "\n",
    "if dataset == 'cifar10':\n",
    "    train_mean = np.array([125.307, 122.950, 113.865])\n",
    "    train_std = np.array([62.993, 62.089, 66.705])\n",
    "    test_mean = np.array([126.025, 123.708, 114.854])\n",
    "    test_std = np.array([62.896, 61.937, 66.706])\n",
    "else:\n",
    "    train_ds = datasets.CIFAR100(os.path.join(DATA_ROOT, 'cifar100'), train=True, download=True)\n",
    "    test_ds = datasets.CIFAR100(os.path.join(DATA_ROOT, 'cifar100'), train=False, download=True)\n",
    "\n",
    "    _data_train = np.concatenate([np.array(train_ds[i][0]) for i in range(len(train_ds))])\n",
    "    _data_test = np.concatenate([np.array(test_ds[i][0]) for i in range(len(test_ds))])\n",
    "\n",
    "    train_mean = _data_train.mean(axis=(0, 1))\n",
    "    train_std = _data_train.std(axis=(0, 1))\n",
    "\n",
    "    test_mean = _data_test.mean(axis=(0, 1))\n",
    "    test_std = _data_test.std(axis=(0, 1))\n",
    "    \n",
    "#     train_mean = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
    "#     train_std = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
    "    \n",
    "#     test_mean = (0.5088964127604166, 0.48739301317401956, 0.44194221124387256)\n",
    "#     test_mean = (0.2682515741720801, 0.2573637364478126, 0.2770957707973042)\n",
    "\n",
    "    print(f'Hard code CIFAR100 train/test mean/std for next time')\n",
    "\n",
    "print('train mean/std:', train_mean, train_std)\n",
    "print('test mean/std:', test_mean, test_std)\n",
    "\n",
    "# Normalize mean std to 0..1 from 0..255\n",
    "train_mean /= 255\n",
    "train_std /= 255\n",
    "test_mean /= 255\n",
    "test_std /= 255\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    # transforms.RandomCrop(32, padding=4),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(train_mean, train_std),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(test_mean, test_std),\n",
    "])\n",
    "\n",
    "# %% Choose model params from dataset\n",
    "\n",
    "if dataset == 'cifar10':\n",
    "    dataloader = datasets.CIFAR10\n",
    "    data_loader_root = os.path.join(DATA_ROOT, 'cifar10')\n",
    "    num_classes = 10\n",
    "    title = 'cifar-10'\n",
    "else:\n",
    "    dataloader = datasets.CIFAR100\n",
    "    data_loader_root = os.path.join(DATA_ROOT, 'cifar100')\n",
    "    num_classes = 100\n",
    "    title = 'cifar-100'\n",
    "\n",
    "# %% Models, criterions, optimizers\n",
    "\n",
    "print(\"==> creating model \")\n",
    "# model = AlexNet(num_classes)\n",
    "#model=resnet.ResNet50()\n",
    "#model=resnext.CifarResNeXt(cardinality=32,depth=4)\n",
    "model=resnext50()\n",
    "# model=DenseNet(growthRate=19)\n",
    "model = model.cuda()\n",
    "\n",
    "# inference_model = torch.nn.DataParallel(inferenece_model).cuda()\n",
    "cudnn.benchmark = True\n",
    "print('\\tTotal params: %.2fM' % (sum(p.numel() for p in model.parameters()) / 1000000.0))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "criterion_attack = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "inference_model = InferenceAttack_HZ(num_classes).cuda()\n",
    "\n",
    "private_train_criterion = nn.MSELoss()\n",
    "\n",
    "optimizer_mem = optim.Adam(inference_model.parameters(), lr=0.00001)\n",
    "\n",
    "# %% Load Dataset from file\n",
    "print(\"==> Loading selected datasets\")\n",
    "batch_privacy = 100\n",
    "trainset = dataloader(root=data_loader_root, train=True, download=True, transform=transform_train)\n",
    "trainloader = data.DataLoader(trainset, batch_size=batch_privacy, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "# TODO check loader for trainloader_private\n",
    "trainset_private = dataloader(root=data_loader_root, train=True, download=True, transform=transform_test)\n",
    "trainloader_private = data.DataLoader(trainset, batch_size=batch_privacy, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "testset = dataloader(root=data_loader_root, train=False, download=False, transform=transform_test)\n",
    "testloader = data.DataLoader(testset, batch_size=batch_privacy, shuffle=True, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ace81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    mkdir_p(checkpoint_path)\n",
    "\n",
    "is_best = False\n",
    "best_acc = 0.0\n",
    "start_epoch = 0\n",
    "\n",
    "# Train and val\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    print(f'\\nEpoch: [{epoch + 1:d} | {EPOCHS:d}] LR: {state[\"lr\"]:f}')\n",
    "\n",
    "    train_enum = enumerate(trainloader)\n",
    "    train_private_enum = enumerate(zip(trainloader_private, testloader))\n",
    "    for i in range(500 // 2):\n",
    "\n",
    "        if epoch > 3:\n",
    "            privacy_loss, privacy_acc = privacy_train(\n",
    "                train_private_enum, model, inference_model, criterion_attack, optimizer_mem, use_cuda, 1\n",
    "            )\n",
    "\n",
    "            train_loss, train_acc = train_privately(\n",
    "                train_enum, model, inference_model, criterion, optimizer, use_cuda, 1, 1\n",
    "            )\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f'Privacy Res: {privacy_acc * 100:.2f}% | {train_acc * 100:.2f}%')\n",
    "\n",
    "            if (i + 1) % 50 == 0:\n",
    "                train_private_enum = enumerate(zip(trainloader_private, testloader))\n",
    "        else:\n",
    "            train_loss, train_acc = train_privately(\n",
    "                train_enum, model, inference_model, criterion, optimizer, use_cuda, 1000, 0\n",
    "            )\n",
    "\n",
    "            break\n",
    "\n",
    "    test_loss, test_acc = test(testloader, model, criterion, use_cuda)\n",
    "\n",
    "    print(f'Test Acc: {test_acc * 100:.2f}%')\n",
    "\n",
    "    # save model\n",
    "    is_best = test_acc > best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epochz': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, False, checkpoint=checkpoint_path, filename='epoch%d' % epoch)\n",
    "\n",
    "print(f'Best acc: {best_acc * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff879c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa2d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GPU if available\n",
    "import utilse\n",
    "\n",
    "utilse.load_checkpoint('E:/KD_MI/experiments/base_cnn/cifar10/resnext/epoch14',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1336f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
